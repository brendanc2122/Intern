{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08072832",
   "metadata": {},
   "source": [
    "### Import Data & Convert Dataframe to Dataset Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b96b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('WELFake_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4194156b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72134 entries, 0 to 72133\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  72134 non-null  int64 \n",
      " 1   title       71576 non-null  object\n",
      " 2   text        72095 non-null  object\n",
      " 3   label       72134 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fc6ce3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a8b7a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     39\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f12183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec102133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72129</th>\n",
       "      <td>WASHINGTON (Reuters) - Hackers believed to be ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72130</th>\n",
       "      <td>You know, because in fantasyland Republicans n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72131</th>\n",
       "      <td>Migrants Refuse To Leave Train At Refugee Camp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72132</th>\n",
       "      <td>MEXICO CITY (Reuters) - Donald Trump’s combati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72133</th>\n",
       "      <td>Goldman Sachs Endorses Hillary Clinton For Pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72095 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      No comment is expected from Barack Obama Membe...      1\n",
       "1         Did they post their votes for Hillary already?      1\n",
       "2       Now, most of the demonstrators gathered last ...      1\n",
       "3      A dozen politically active pastors came here f...      0\n",
       "4      The RS-28 Sarmat missile, dubbed Satan 2, will...      1\n",
       "...                                                  ...    ...\n",
       "72129  WASHINGTON (Reuters) - Hackers believed to be ...      0\n",
       "72130  You know, because in fantasyland Republicans n...      1\n",
       "72131  Migrants Refuse To Leave Train At Refugee Camp...      0\n",
       "72132  MEXICO CITY (Reuters) - Donald Trump’s combati...      0\n",
       "72133  Goldman Sachs Endorses Hillary Clinton For Pre...      1\n",
       "\n",
       "[72095 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca7237",
   "metadata": {},
   "source": [
    "### Split Train-Test-Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90329eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 1\n",
    "train, test = train_test_split(data, test_size=0.3, shuffle = True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08cb37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 2\n",
    "train, val = train_test_split(train, test_size=0.3, shuffle = True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac43e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the Train/Test/Val \n",
    "train_data = datasets.DatasetDict({'train': datasets.Dataset.from_pandas(train, preserve_index=False),\n",
    "                                   'test':datasets.Dataset.from_pandas(test,  preserve_index=False), \n",
    "                                   'validation':datasets.Dataset.from_pandas(val,  preserve_index=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a65928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 35326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 21629\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 15140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be2ab1",
   "metadata": {},
   "source": [
    "### Tokenize  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5da8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e543571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(row):\n",
    "    token = tokenizer(row[\"text\"], truncation=True)\n",
    "    row[\"input_ids\"] = token[\"input_ids\"]\n",
    "    row[\"attention_mask\"] = token[\"attention_mask\"]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdeb449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf473b4b3664f09afa09133edee2202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35326 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afb3f8f3d334999ba982bfef468b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21629 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d401210878a543cd9a5513b2cf8c96b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15140 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "626ad789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 21629\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2712a4",
   "metadata": {},
   "source": [
    "### Transformers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bdbb35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d4dd49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x0000027B3AA6F940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x0000027B3AA6F940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x0000027B3AA6FAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x0000027B3AA6FAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x0000027B3A9AD1F0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x0000027B3A9AD1F0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x0000027B3A9AD790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x0000027B3A9AD790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = train_data['train'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = train_data[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b55a9bcb",
   "metadata": {},
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(train_data[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebfeb51e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c2790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e93f7e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec1dfeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "intermediate_layer (Dense)      (None, 512)          393728      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            1026        intermediate_layer[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 108,705,026\n",
      "Trainable params: 108,705,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b207dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efb3635d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8074c4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2736 _minimize\n        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:562 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1270 _filter_grads\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['tf_bert_model_2/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_2/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_2/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_2/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_2/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'intermediate_layer/kernel:0', 'intermediate_layer/bias:0', 'output_layer/kernel:0', 'output_layer/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_validation_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:814\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    816\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2828\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\u001b[38;5;241m.\u001b[39m_filtered_call(args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3210\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(\n\u001b[0;32m   3201\u001b[0m     status\u001b[38;5;241m=\u001b[39mag_status, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autograph_options):\n\u001b[0;32m   3202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3205\u001b[0m   \u001b[38;5;66;03m# and 2. there's no provided input signature\u001b[39;00m\n\u001b[0;32m   3206\u001b[0m   \u001b[38;5;66;03m# and 3. there's been a cache miss for this calling context\u001b[39;00m\n\u001b[0;32m   3207\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_relax_shapes\n\u001b[0;32m   3208\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m call_context_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed):\n\u001b[1;32m-> 3210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_define_function_with_shape_relaxation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3212\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[0;32m   3213\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_graph_function(args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3141\u001b[0m, in \u001b[0;36mFunction._define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3134\u001b[0m   (relaxed_arg_specs, relaxed_kwarg_specs) \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   3135\u001b[0m       (args, kwargs), relaxed_arg_specs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3136\u001b[0m   (args, kwargs) \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   3137\u001b[0m       (relaxed_arg_specs, relaxed_kwarg_specs),\n\u001b[0;32m   3138\u001b[0m       nest\u001b[38;5;241m.\u001b[39mflatten((args, kwargs), expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   3139\u001b[0m       expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 3141\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelaxed_arg_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39marg_relaxed[rank_only_cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, args, kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3065\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3060\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3061\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3062\u001b[0m ]\n\u001b[0;32m   3063\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3064\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3076\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3077\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3078\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3079\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3080\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3082\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 986\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m    991\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:600\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# We register a variable creator with reduced priority. If an outer\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m# variable creator is just modifying keyword arguments to the variable\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# constructor, this will work harmoniously. Since the `scope` registered\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;66;03m# better than the alternative, tracing the initialization graph but giving\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# the user a variable type they didn't want.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m--> 600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:973\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    972\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    974\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2736 _minimize\n        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:562 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\brend\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1270 _filter_grads\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['tf_bert_model_2/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model_2/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model_2/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model_2/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model_2/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'intermediate_layer/kernel:0', 'intermediate_layer/bias:0', 'output_layer/kernel:0', 'output_layer/bias:0'].\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tf_train_set, validation_data=tf_validation_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
